---
title: "Introducing biClassify"
author: Alexander F. Lapanowski and Irina Gaynanova
output: pdf_document
toc: true
number_sections: true
vignette: >
  %\VignetteIndexEntry{biClassify}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

\section{Introduction}
\texttt{biClassify} is a package for adapting Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Kernel Discriminant Analysis to a variety of situations where the conventional methods may not work. In particular, this package has methodology for the following problems:

\begin{enumerate}
\item Linear and Quadratic classification in the large-sample case with small-to-medium sized number of features. The available compressed LDA and QDA methods provide alternatives to random sub-sampling which are shown to produce lower mean misclassification error rates and lower standard error in the misclassification error rates [Lapanowski and Gaynanova] (preprint, 2020).
\item Kernel classification where the data has a medium-to-large number of features. In this case, one would like to learn a non-linear decision boundary and have simultaneous sparse feature selection. The sparse kernel discriminant analysis method provided, Sparse Kernel Optimal Scoring, is presented in Lapanowski and Gaynanova, 2019.
\end{enumerate}


\section{Quick Start}
The purpose of this section is to give the user a 
quick overview of the package and the types of 
problems it can be used to solve. Accordingly, we implement only the basic versions of the available methods, and more detailed presentations are given in later sections.

We first load the package
```{r}
library(biClassify)
```
\subsection{Quick LDA Example}
Our first example illustrates the compressed LDA function on data well-suited for LDA. The first two features of the training data in \texttt{LDA_Data} are plotted below:
```{r}
data(LDA_Data)
```
```{r, echo = FALSE, fig.height=4, fig.width=5, fig.align = "center"}
plot(LDA_Data$TrainData[,2]~LDA_Data$TrainData[,1],
     col = c("orange","blue")[LDA_Data$TrainCat],
     pch = c("1","2")[LDA_Data$TrainCat],
     xlab = "Feature 1", 
     ylab = "Feature 2",
     main = "Scatterplot of LDA Training Data")
```

This data set has $n= 10,000$ training samples with $p=10$ features. It is normally distributed, and the two classes have equal covariance matrices. The test data was independently generated from the same distribution, but it has only $n=1,000$ samples. 

Let us use compressed LDA to predict the test data labels.
```{r}
test_pred <- lda(TrainData = LDA_Data$TrainData,
                 TrainCat = LDA_Data$TrainCat,
                 TestData = LDA_Data$TestData,
                 Method = "Compressed")
mean(test_pred != LDA_Data$TestCat)
```
The automatic impementation of compressed LDA predicted the Test labels perfectly! However, this is due, in part, to the classes being well-separated and having the same covariance structure. Let us now consider an example of where LDA will not perform well.

\subsection{Quick QDA Example}
Our next example illustrates the compressed QDA function on data well-suited for QDA. The first two features of the training data in \texttt{QDA_Data} are plotted below:

```{r}
data(QDA_Data)
```
```{r, echo = FALSE, fig.height=4, fig.width=5, fig.align = "center"}
plot(QDA_Data$TrainData[,2]~QDA_Data$TrainData[,1],
     col = c("orange","blue")[QDA_Data$TrainCat],
     pch = c("1","2")[QDA_Data$TrainCat],
     xlab = "Feature 1", 
     ylab = "Feature 2",
     main = "Scatterplot of QDA Training Data")
```

A modification of Quadratic Discriminant Analysis is well-suited to such data. The package comes with a function \texttt{qda} for such purposes.
```{r}
test_pred <- qda(TrainData = QDA_Data$TrainData,
                 TrainCat = QDA_Data$TrainCat,
                 TestData = QDA_Data$TestData,
                 Method = "Compressed")
mean(test_pred != QDA_Data$TestCat)
```

Compressed QDA gives perfect class prediction

\subsection{Quick Sparse Kernel Optimal Scoring Example}
What happens if the data is not well-suited to either Linear or Quadratic Discriminant Analysis? Moreover, what happens if, in addtion to a non-linear decision boundary between classes, there also appear to be variables which do not contribute to group separation?

For example, consider the \texttt{KOS_Data} shown below.
```{r}
data(KOS_Data)
```

```{r,echo = FALSE, fig.height=4, fig.width=8, fig.align = "center"}
par(mfrow = c(1,2))
plot(KOS_Data$TrainData[,2]~KOS_Data$TrainData[,1], col = c("orange","blue")[KOS_Data$TrainCat],
     pch = c("1","2")[KOS_Data$TrainCat],
     xlab = "Feature 1",
     ylab = "Feature 2",
     main = "True Features")
plot(KOS_Data$TrainData[,4]~KOS_Data$TrainData[,3], col = c("orange","blue")[KOS_Data$TrainCat],
     pch = c("1","2")[KOS_Data$TrainCat],
     xlab = "Feature 3",
     ylab = "Feature 4",
     main = "Noise Features")
par(mfrow = c(1,1))
```
For this data set, neither lda or qda would suffice. The function \texttt{KOS} is the sparse kernel optimal scoring algorithm presented in Lapanowski, and Gaynanova (2019). It is particularly well-suited to such problems, as can be seen from the following.

```{r, fig.height=4, fig.width=5, fig.align = "center"}
output <- KOS(TrainData = KOS_Data$TrainData, 
              TrainCat = KOS_Data$TrainCat,
              TestData = KOS_Data$TestData)
print(output$Weight)
mean(output$Predictions != KOS_Data$TestCat)
plot(output$Dvec,
     main = "Discriminant Vector Coefficients", 
     xlab = "Feature Index",
     ylab="Discriminant Coefficient Value")
```
\texttt{Weight} in the output is how much weight 
the kernel classifier gives to each feature. The weight values lie in $[-1,1]$, and zero weight means that the feature does not contribute to computing the discriminant function. The KOS function correctly identifies that the first two features are important for class separation, and gives them full weight. It also correctly identifies Features 3 and 4 as being ``noise'', and it gives them zero weight.

\texttt{Predictions} are the predicted class labels for the test data. As we can see, \texttt{KOS} has perfect classification. 

\texttt{Dvec} are the coefficients of the kernel classifier. 



\section{Compressed Linear Discriminant Analysis}
This section provides a more in-depth treatment to the Linear Discriminant methods available in \texttt{biClassify}.

There are five seperate linear discriminant methods avilable through the \texttt{lda} wrapper function:
\begin{enumerate}
\item \texttt{Full} Linear Discriminant Analsysis, which is LDA trained on the full data
[Cite Mardia or Elements of Stat. Learning].
\item \texttt{Compressed} Linear Discriminant Analysis [Lapanowski, Gaynanova] (2020, preprint).
\item \texttt{Projected} LDA [Lapanowski, Gaynanova] (2020, preprint)
\item \texttt{Subsampled} LDA, where LDA is trained on data which is sub-sampled uniformly from both classes.
\item \texttt{FashRandomFisher} Discriminant Analysis as in [Ye, et. al.] (2017).
\end{enumerate}

The individual methods are invoked by setting the \texttt{Method} argument. Let us first load the data for notational convenience.

```{r}
TrainData <- LDA_Data$TrainData
TrainCat <- LDA_Data$TrainCat
TestData <- LDA_Data$TestData
TestCat <- LDA_Data$TestCat
```

\subsection{Full LDA}
This method is the result of setting \texttt{Method} equal to \texttt{"Full"}.
This method is traditional Linear Discriminant Analysis, as presented in [Citation]. No additional parameters need to be supplied, and the code will run as stated.

```{r}
test_pred <- lda(TrainData, TrainCat, TestData)
table(test_pred)
mean(test_pred != TestCat)
```
which produces a list containing a vector of predicted class labels for \texttt{TestData} and the discriminant vector used in LDA.

\subsection{Compressed LDA}
Compressed LDA seeks to solve the LDA problem on reduced-size data. It first compressed the groups of centered data $(X^{g} - \overline{X}_g)$ via a compression matrix $Q^{g}$. The entries $Q_{i,j}^{g}$ are i.i.d. sparse radamacher random variables with distribution 
$$
\mathbb{P}(Q^{g}_{i,j}=1) = \mathbb{P}(Q^{g}_{i,j}=-1)= \frac{p}{2}\text{ and } \mathbb{P}(Q^{g}_{i,j}=0) = 1-p.
$$




This method is the result of setting \texttt{Method} equal to \verb+"Compressed"+. It is compressed LDA, as presented in Lapanowski, Gaynanova (2020), preprint. Compressed LDA reduces the group sample amounts from $n_1$ and $n_2$ to $m_1$ and $m_2$ respectively.

Compressed LDA requires the parameters \texttt{m1}, \texttt{m2}, \texttt{s}.


The easiest way to run Compressed LDA is to set \texttt{Mode} to \texttt{Automatic} and not worry about supplying additional parameters.
```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Compressed", Mode = "Automatic")
table(test_pred)
mean(test_pred != TestCat)
```

\texttt{Automatic} is the default value for \texttt{Mode}, and so one could simply run

```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Compressed")
table(test_pred)
mean(test_pred != TestCat)
```
and obtain the same output.


When \texttt{Mode} is set to \texttt{Interactive}, prompts will appear asking for the compression amounts $m_1$, $m_2$, and sparsity level $s$ to be used in compression. The user will type in the amounts:
```{r, eval=FALSE}
output <- lda(TrainData, TrainCat, TestData, Method = "Compressed", Mode = "Interactive")
"Please enter the number m1 of group 1 compression samples: "700
"Please enter the number m2 of group 2 compression samples: "300
"Please enter sparsity level s used in compression: "0.01
```
and the output is produced. 

\subsubsection{Mode - Research}
If the user is interested in running simulation studies or has mastery over the functionality, they may wish to give the \texttt{lda} function all parameters. 

```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Compressed", 
              Mode = "Research", m1 = 700, m2 = 300, s = 0.01)

table(test_pred)
mean(test_pred != TestCat)
```


WARNING: The argument \texttt{Mode} will override any supplied parameters if its value is \texttt{Automatic} or \texttt{Research}.

\subsection{Sub-Sampled LDA}
Sub-sampled LDA is just LDA trained on data sub-sampled uniformly from both classes. 

To run sub-sampled LDA, set \texttt{Method} equal to \texttt{Subsampled}. It requires the additional parameters \texttt{m1} and \texttt{m2}.

\subsubsection{Mode - Automatic}
The easiest way to run Compressed LDA is to set \texttt{Mode} to \texttt{Automatic} and not worry about supplying additional parameters.
```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Subsampled", Mode = "Automatic")
table(test_pred)
```

\texttt{Automatic} is the default value for \texttt{Mode}, and so one could simply run

```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Subsampled")
table(test_pred)
```
and obtain the same output.

\subsubsection{Mode - Interactive} 
When \texttt{Mode} is set to \texttt{Interactive}, prompts will appear asking for the sub-sample amounts $m_1$, $m_2$ for each group to be used. The user will type in the amounts:
```{r, eval=FALSE}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Subsampled", Mode = "Interactive")
"Please enter the number m1 of group 1 sub-samples: "700
"Please enter the number m2 of group 2 sub-samples: "300
```
and the output is produced. 


\subsubsection{Mode - Research}
If the user is interested in running simulation studies or has mastery over the functionality, they may wish to give the \texttt{lda} function all parameters. 

```{r}
output <- lda(TrainData, TrainCat, TestData, Method = "Subsampled", 
              Mode = "Research", m1 = 700, m2 = 300)

table(output)
mean(output != TestCat)
```


WARNING: The argument \texttt{Mode} will override any supplied parameters if its value is \texttt{Automatic} or \texttt{Research}.


\subsection{Projected LDA}
This method is the result of setting \texttt{Method} equal to \verb+"PRojected"+. It is Projected LDA, as presented in Lapanowski, Gaynanova (2020), preprint. Projected LDA creates the discriminant vector on compressed data and then projects the full training data onto the discriminant vector.

Projected LDA requires the parameters \texttt{m1}, \texttt{m2}, \texttt{s}.

\subsubsection{Mode - Automatic}

The easiest way to run Projected LDA is to set \texttt{Mode} to \texttt{Automatic} and not worry about supplying additional parameters.
```{r}
output <- lda(TrainData, TrainCat, TestData, Method = "Projected", Mode = "Automatic")
table(output)
mean(output != TestCat)
```

\texttt{Automatic} is the default value for \texttt{Mode}, and so one could simply run

```{r}
output <- lda(TrainData, TrainCat, TestData, Method = "Projected")
table(output)
mean(output != TestCat)
```
and obtain the same output.


\subsubsection{Mode - Interactive} 
When \texttt{Mode} is set to \texttt{Interactive}, prompts will appear asking for the compression amounts $m_1$, $m_2$, and sparsity level $s$ to be used in compression. The user will type in the amounts:
```{r, eval=FALSE}
output <- lda(TrainData, TrainCat, TestData, Method = "Projected", Mode = "Interactive")
"Please enter the number m1 of group 1 compression samples: "700
"Please enter the number m2 of group 2 compression samples: "300
"Please enter sparsity level s used in compression: "0.01

```
and the output is produced. 

\subsubsection{Mode - Research}
If the user is interested in running simulation studies or has mastery over the functionality, they may wish to give the \texttt{lda} function all parameters. 

```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "Projected", 
              Mode = "Research", m1 = 700, m2 = 300, s = 0.01)

table(test_pred)
mean(output != TestCat)
```


WARNING: The argument \texttt{Mode} will override any supplied parameters if its value is \texttt{Automatic} or \texttt{Research}.




\subsection{Fast Random Fisher Discriminant Analysis}

This method is the result of setting \texttt{Method} equal to \verb+"fastRandomFisher"+. It is the Fast Random Fisher Discriminant Analysis algorithm, as presented in Ye, et. al. (2017). Fast Random fisher creates the discriminant vector on reduced sample amounts $m$, and then projects the full training data onto the learned discriminant vector. The difference between Fast Random Fisher Discriminant Analysis and Projected LDA is that Fast Random Fisher mixes the groups together when forming the discriminant vector, but Projected LDA does not.

Fast Random Fisher requires the parameters \texttt{m}, and \texttt{s}.

\subsubsection{Mode - Automatic}

The easiest way to run Fast Random Fisher is to set \texttt{Mode} to \texttt{Automatic} and not worry about supplying additional parameters.
```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "fastRandomFisher", Mode = "Automatic")
table(test_pred)
mean(test_pred != TestCat)
```

\texttt{Automatic} is the default value for \texttt{Mode}, and so one could simply run

```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "fastRandomFisher")
table(test_pred)
mean(test_pred != TestCat)
```
and obtain the same output.


\subsubsection{Mode - Interactive} 
When \texttt{Mode} is set to \texttt{Interactive}, prompts will appear asking for the total amount of compressed samples $m$ and sparsity level $s$ to be used in compression. The user will type in the amounts:
```{r, eval=FALSE}
output <- lda(TrainData, TrainCat, TestData, Method = "fastRandomFisher", Mode = "Interactive")
"Please enter the number m of total compressed samples: "1000
"Please enter sparsity level s used in compression: "0.01
```
and the output is produced. 

\subsubsection{Mode - Research}
If the user is interested in running simulation studies or has mastery over the functionality, they may wish to give the \texttt{lda} function all parameters. 

```{r}
test_pred <- lda(TrainData, TrainCat, TestData, Method = "fastRandomFisher", 
              Mode = "Research", m = 1000, s = 0.01)

table(test_pred)
mean(test_pred != TestCat)
```


WARNING: The argument \texttt{Mode} will override any supplied parameters if its value is \texttt{Automatic} or \texttt{Research}.



\section{Quadratic Discriminant Analysis}
This section provides a more in-depth treatment to the Linear Discriminant methods available in \texttt{biClassify}.

There are three seperate quadratic discriminant methods avilable through the \texttt{qda} wrapper function:
\begin{enumerate}
\item \texttt{Full} Quadratic Discriminant Analsysis, which is LDA trained on the full data
[Cite Mardia or Elements of Stat. Learning].
\item \texttt{Compressed} Linear Discriminant Analysis [Lapanowski, Gaynanova] (2020, preprint).
\item \texttt{Subsampled} LDA, where LDA is trained on data which is sub-sampled uniformly from both classes.
\end{enumerate}

The individual methods are invoked by setting the \texttt{Method} argument. Let us first load the data for notational convenience.

```{r}
TrainData <- QDA_Data$TrainData
TrainCat <- QDA_Data$TrainCat
TestData <- QDA_Data$TestData
TestCat <- QDA_Data$TestCat
```


\subsection{Full QDA}
This method is the result of setting \texttt{Method} equal to \texttt{"Full"}.
This method is traditional Quadratic Discriminant Analysis, as presented in [Citation]. No additional parameters need to be supplied, and the code will run as stated. Unlike the \texttt{lda} function, only the class predictions are produced:

```{r}
Predictions <- qda(TrainData, TrainCat, TestData, Method = "Full")
summary(Predictions)
```

\subsection{Compressed QDA}
This method is the result of setting \texttt{Method} equal to \verb+"Compressed"+. It is compressed QDA, as presented in Lapanowski, Gaynanova (2020), preprint. Compressed QDA reduces the group sample amounts from $n_1$ and $n_2$ to $m_1$ and $m_2$ respectively via compression and trains QDA on the reduced samples.

Compressed QDA requires the parameters \texttt{m1}, \texttt{m2}, \texttt{s}.

\subsubsection{Mode - Automatic}

The easiest way to run Compressed QDA is to set \texttt{Mode} to \texttt{Automatic} and not worry about supplying additional parameters.
```{r}
output <- qda(TrainData, TrainCat, TestData, Method = "Compressed", Mode = "Automatic")
summary(output)
```

\texttt{Automatic} is the default value for \texttt{Mode}, and so one could simply run

```{r}
output <- qda(TrainData, TrainCat, TestData, Method = "Compressed")
summary(output)
```
and obtain the same output.


\subsubsection{Mode - Interactive} 
When \texttt{Mode} is set to \texttt{Interactive}, prompts will appear asking for the compression amounts $m_1$, $m_2$, and sparsity level $s$ to be used in compression. The user will type in the amounts:
```{r, eval=FALSE}
output <- qda(TrainData, TrainCat, TestData, Method = "Compressed", Mode = "Interactive")
"Please enter the number m1 of group 1 compression samples: "700
"Please enter the number m2 of group 2 compression samples: "300
"Please enter sparsity level s used in compression: "0.01

summary(output)
```
and the output is produced. 

\subsubsection{Mode - Research}
If the user is interested in running simulation studies or has mastery over the functionality, they may wish to give the \texttt{qda} function all parameters. 

```{r}
output <- qda(TrainData, TrainCat, TestData, Method = "Compressed", 
              Mode = "Research", m1 = 700, m2 = 300, s = 0.01)

summary(output)
```

\subsection{Sub-Sampled QDA}
Sub-sampled QDA is just QDA trained on data sub-sampled uniformly from both classes. To run sub-sampled QDA, set \texttt{Method} equal to \texttt{Subsampled}.

It requires the additional parameters \texttt{m1} and \texttt{m2}.

\subsubsection{Mode - Automatic}
The easiest way to run Compressed LDA is to set \texttt{Mode} to \texttt{Automatic} and not worry about supplying additional parameters.
```{r}
output <- qda(TrainData, TrainCat, TestData, Method = "Subsampled", Mode = "Automatic")
summary(output)
```

\texttt{Automatic} is the default value for \texttt{Mode}, and so one could simply run

```{r}
output <- qda(TrainData, TrainCat, TestData, Method = "Subsampled")
summary(output)
```
and obtain the same output.

\subsubsection{Mode - Interactive} 
When \texttt{Mode} is set to \texttt{Interactive}, prompts will appear asking for the sub-sample amounts $m_1$, $m_2$ for each group to be used. The user will type in the amounts:
```{r, eval=FALSE}
output <- qda(TrainData, TrainCat, TestData, Method = "Subsampled", Mode = "Interactive")
"Please enter the number m1 of group 1 sub-samples: "700
"Please enter the number m2 of group 2 sub-samples: "300

summary(output)
```
and the output is produced. 


\subsubsection{Mode - Research}
If the user is interested in running simulation studies or has mastery over the functionality, they may wish to give the \texttt{qda} function all parameters. 

```{r}
output <- qda(TrainData, TrainCat, TestData, Method = "Subsampled", 
              Mode = "Research", m1 = 700, m2 = 300)

summary(output)
```


WARNING: The argument \texttt{Mode} will override any supplied parameters if its value is \texttt{Automatic} or \texttt{Research}.


\section{Sparse Kernel Discriminant Analysis}

This section presents the various kernel optimal scoring methods available in the biClassify package. Kernel optimal scoring, as presented in [Lapanowski and Gaynanova] Artificial Intelligence and Statistics, 2019. It is an equivalent form of kernel discriminant analysis.
















\section{Extended Example - Classifying Even and Odd Digits in the MNIST Data}
This section presents the methods in the \texttt{biClassify} package in more detail by working through a more complicated example - labelling the even and odd digits in the biclassify package 













\newpage
\newpage

\section{Linear Discriminant Methods}
This section presents the various linear discriminant methods available in the biClassify package.

\hspace{.2in}
\hrule
\vspace{.1in}
\begin{center}
\texttt{lda}\hspace{1in} \emph{Linear Discriminant Classifiers}
\end{center}
\vspace{.1in}
\hrule

\hspace{.2in}

\textbf{Usage}

\begin{verbatim}
  lda(TrainData, TrainCat, TestData, Method = "Full", ...)
\end{verbatim}


\subsection{Arguments}
\begin{itemize}
\item \texttt{TrainData} - A (n x p) matrix of training samples; n rows of samples with p features in the columns.
\item \texttt{TrainCat} - A vector of length n corresponding to class labels (1 or 2) of the n samples in \texttt{TrainData}.
\item \texttt{TestData} - A (m x p) matrix of test samples; m rows of samples with p features in the column.
\item \texttt{Method} - A string of characters indicating which linear discriminant method to use. Acceptable values are \texttt{"Full"}, \verb+"Compressed"+, \texttt{"Subsampled"}, \verb+"Projected"+, and \verb+"fastRandomFisher"+.
\item \texttt{Mode} - A string of characters determining how the user interacts with the reduced sample parameters. Acceptable values are 
\texttt{Automatic}, \texttt{Interactive}, or \texttt{Research}.
\item \texttt{m} The total number of mixed compressed samples used when \texttt{Method} equals \texttt{fastRandomFisher}.
\item \texttt{m1} The total number of group $1$ reduced samples used when \texttt{Method} equals \texttt{Compressed}, \texttt{Projected}, or \texttt{Subsampled}.
\item \texttt{m2} The total number of group $2$ reduced samples used when \texttt{Method} equals \texttt{Compressed}, \texttt{Projected}, or \texttt{Subsampled}.
\item \texttt{s} Sparsity level used when generating the compressed samples when \texttt{Method} equals \text{Compressed}, \text{Projected}, and \texttt{fastRandomFisher}. Must be between $0$ and $1$.
\item \texttt{gamma} - Regularization amount used in stabilizing the sample covariance matrix. A non-negative real number. Must be non-negative and default value is $1E-5$.
\end{itemize}

\subsection{LDA Data}

\textbf{Usage}

\begin{verbatim}
LDA_Data
\end{verbatim}

This package comes with a data set appropriate for applying the linear discriminant methods. It comes in a list consisting of \texttt{TrainData}, \texttt{TrainCat}, \texttt{TestData}, and \texttt{TestCat}. There are two uneven classes of $10$-dimensional normally distributed data. Class $1$ has $n_1=7000$ samples, with class mean equal to $-2\times \mathbf{1}\in \mathbb{R}^{10}$ (the vector of all $-2$). Class $2$ has $n_2=3000$ samples with class mean $2 \times \mathbf{1}\in \mathbb{R}^{10}$ (the vector of all $2$). The shared covariance matrix $\Sigma$ of the two classes has $(i,j)$ coordinate $0.5^{|i-j|}$. The data is loaded and the first two features of the training data are plotted below.


```{r, fig.height=4, fig.width=5, fig.align = "center"}
TrainData <- LDA_Data$TrainData
TrainCat <- LDA_Data$TrainCat
TestData <- LDA_Data$TestData
TestCat <- LDA_Data$TestCat
plot(TrainData[,2]~TrainData[,1], col = c("orange","blue")[TrainCat],
     pch = c("1","2")[TrainCat])
```

\subsection{Handling Sample Parameters}
Versions of the \texttt{lda} function, as specified by the parameter \texttt{Method}, have different parameters for reduced-sample amounts. The user can handle these parameters in three seperate ways:
\begin{enumerate}
\item Automatically - this is when the argument \texttt{Mode} is set to \texttt{Automatic}. All necessary parameters for reducing the sample amounts will be automatically generated depending on the particular version of lda as specified by \texttt{Method}. This is the default value of \texttt{Mode} and is recommended for first-time users.
\item Interactive - this is when the argument \texttt{Mode} is set to \texttt{Interactive}. Interactive prompts will appear asking for the user to input the values of all required parameters for reducing the sample amounts depending on the particular version of lda as specified by \texttt{Method}. This is the reccomended version for those somewhat familiar with the functionality and wishing to learn more.

\item Research - this is when the argument \texttt{Mode} is set to \texttt{Research}. The user must specify all necessary parameters to the function \texttt{lda} depending on the particular version of lda as specified by \texttt{Mode}. This is the version reccomended for simulation studies.
\end{enumerate}
All three versions of the \texttt{Mode} option will be featured for the different lda methods.








\newpage
\section{Quadratic Discriminant Methods}
This section presents the various quadratic discriminant methods available in the biClassify package.

\hspace{.2in}
\hrule
\vspace{.1in}
\begin{center}
\texttt{qda}\hspace{1in} \emph{Quadratic Discriminant Classifiers}
\end{center}
\vspace{.1in}
\hrule

\hspace{.2in}

\textbf{Usage}

\begin{verbatim}
  qda(TrainData, TrainCat, TestData, Method = "Full", ... )
\end{verbatim}


\subsection{Arguments}
\begin{itemize}
\item \texttt{TrainData} - A (n x p) matrix of training samples; n rows of samples with p features in the columns.
\item \texttt{TrainCat} - A vector of length n corresponding to class labels (1 or 2) of the n samples in \texttt{TrainData}.
\item \texttt{TestData} - A (m x p) matrix of test samples; m rows of samples with p features in the column.
\item \texttt{Method} - A string of characters indicating which linear discriminant method to use. Acceptable values are \texttt{"Full"}, \verb+"Compressed"+, and \texttt{"Subsampled"}.
\item \texttt{Mode} - A string of characters determining how the user interacts with the reduced sample parameters. Acceptable values are 
\texttt{Automatic}, \texttt{Interactive}, or \texttt{Research}.
\item \texttt{m1} The total number of group $1$ reduced samples used when \texttt{Method} equals \texttt{"Compressed"} or \texttt{Subsampled}.
\item \texttt{m2} The total number of group $2$ reduced samples used when \texttt{Method} equals \texttt{"Compressed"} or \texttt{"Subsampled"}.
\item \texttt{s} Sparsity level used when generating the compressed samples when \texttt{Method} equals \text{Compressed}. Must be between $0$ and $1$.
\item \texttt{gamma} - Regularization amount used in stabilizing the sample covariance matrices for the two classes. A non-negative real number.
\end{itemize}


\subsection{QDA Data}

\textbf{Usage}

\begin{verbatim}
QDA_Data
\end{verbatim}

This package comes with a data set appropriate for applying the quadratic discriminant methods. It comes in a list consisting of \texttt{TrainData}, \texttt{TrainCat}, \texttt{TestData}, and \texttt{TestCat}. There are two uneven classes of $10$-dimensional normally distributed data. Class $1$ has $n_1=7000$ samples, with class mean equal to $-2\times \mathbf{1}\in \mathbb{R}^{10}$ (the vector of all $-2$). Class $2$ has $n_2=3000$ samples with class mean $2 \times \mathbf{1}\in \mathbb{R}^{10}$ (the vector of all $2$). The classes have unequal covariance structure. The class $1$ covariance matrix $\Sigma_{1}$ has $(i,j)$ coordinate equal to $0.5^{|i-j|}$, while the class $2$ covariance matrix $\Sigma_{2}$ has $(i,j)$ equal to $(-0.5)^{|i-j|}$. The data is loaded and the first two features of the training data are plotted below.


```{r, fig.height=4, fig.width=5, fig.align = "center"}
TrainData <- QDA_Data$TrainData
TrainCat <- QDA_Data$TrainCat
TestData <- QDA_Data$TestData
TestCat <- QDA_Data$TestCat
plot(TrainData[,2]~TrainData[,1], col = c("orange","blue")[TrainCat],
     pch = c("1","2")[TrainCat])
```


\section{Sparse Kernel Optimal Scoring}
\subsection{KOS Data}
This package comes with a data set useful for exploring the kernel optimal scoring functionality. 

```{r, fig.height=4, fig.width=8, fig.align = "center"}
TrainData <- KOS_Data$TrainData
TrainCat <- KOS_Data$TrainCat
TestData <- KOS_Data$TestData
TestCat <- KOS_Data$TestCat
par(mfrow = c(1,2))
plot(TrainData[,2]~TrainData[,1], col = c("orange","blue")[TrainCat],
     pch = c("1","2")[TrainCat])
plot(TrainData[,4]~TrainData[,3], col = c("orange","blue")[TrainCat],
     pch = c("1","2")[TrainCat])
par(mfrow = c(1,1))
```



\subsection{Select Parameters}
This package comes with an automated method for selecting the kernel optimal scoring parameters.

\hspace{.2in}
\hrule
\vspace{.1in}
\begin{center}
\texttt{SelectParams}\hspace{1in} \emph{Automatic Parameter Selection Method}
\end{center}
\vspace{.1in}
\hrule

\hspace{.2in}

\textbf{Usage}

\begin{verbatim}
  SelectParams(TrainData, TrainCat, Sigma = NULL, Gamma = NULL)
\end{verbatim}


\subsection{Arguments}
\begin{itemize}
\item \texttt{TrainData} - A (n x p) matrix of training samples; n rows of samples with p features in the columns.
\item \texttt{TrainCat} - A vector of length n corresponding to class labels (1 or 2) of the n samples in \texttt{TrainData}.
\item \texttt{Sigma} - Gaussian Kernel parameter. Must be strictly greater than $0$. Default value is \texttt{NULL}.
\item \texttt{Gamma} - Ridge penalty on the kernel discriminant function. Must be strictly greater than $0$. Default value is \texttt{NULL}.
\item \texttt{Lambda} - Sparsity penalty on the feature weights. Must be strictly greater than $0$. Default value is \texttt{NULL}.
\end{itemize}


\subsubsection{Hierarchical Parameters}

Sparse kernel optimal scoring has three parameters: a Gaussian kernel parameter `Sigma`, a ridge parameter `Gamma`, and a sparsity parameter `Lambda`. They have a hierarchical dependency, in that `Sigma` influences `Gamma`, and both influence `Lambda`. The ordering is 

Top     `Sigma`

Middle  `Gamma`

Bottom  `Lambda`

When using either of the functions, the user is only allowed to specify parameter combinations which adhere to the hierarchical ordering above. That is, they can only input parameters which go from Top to Bottom. For example, they could specify both `Sigma` and `Gamma`, but leave `Lambda` as the default `NULL` value. On the other hand, the user would not be allowed to specify only `Lambda` while leaving `Sigma` and `Gamma` as their default `NULL` values.

```{r}
SelectParams(TrainData, TrainCat, Sigma = 1, Gamma = 0.1)
```

If the user supplies parameter values which violate the hierarchical ordering, the error message ``Hierarchical order of parameters violated.`` will be returned.
```{r, error = TRUE}
SelectParams(TrainData, TrainCat, Gamma = 0.1)
```

\subsection{KOS}

This package comes with an all-purpose function for running kernel optimal scoring.

\hspace{.2in}
\hrule
\vspace{.1in}
\begin{center}
\texttt{KOS}\hspace{1in} \emph{Kernel Optimal Scoring}
\end{center}
\vspace{.1in}
\hrule

\hspace{.2in}

\textbf{Usage}

\begin{verbatim}
  KOS(TestData, TrainData, TrainCat, Sigma = NULL, Gamma = NULL, Lambda = NULL)
\end{verbatim}


\subsection{Arguments}
\begin{itemize}
\item \texttt{TestData} - A (m x p) matrix of test samples to be classified. Has m rows of samples each with p features.
\item \texttt{TrainData} - A (n x p) matrix of training samples; n rows of samples with p features in the columns.
\item \texttt{TrainCat} - A vector of length n corresponding to class labels (1 or 2) of the n samples in \texttt{TrainData}.
\item \texttt{Sigma} - Gaussian Kernel parameter. Must be strictly greater than $0$. Default value is \texttt{NULL}.
\item \texttt{Gamma} - Ridge penalty on the kernel discriminant function. Must be strictly greater than $0$. Default value is \texttt{NULL}.
\item \texttt{Lambda} - Sparsity penalty on the feature weights. Must be strictly greater than $0$. Default value is \texttt{NULL}.
\end{itemize}

```{r}
Sigma <- 1.325386  
Gamma <- 0.07531579 
Lambda <- 0.002855275

output <- KOS(TestData, TrainData, TrainCat, Sigma = Sigma, 
              Gamma = Gamma, Lambda = Lambda)
print(output$Weight)
summary(output$Predictions)
summary(output$Dvec)
```
